{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends Win Chance Prediction\n",
    "### ML model to predict the outcome of a League of Legends match based on champion selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "League of Legends, often abbreviated as LoL, is a popular online multiplayer video game. It's a competitive 5 versus 5 team-based game in which players control unique champions with special abilities and work together to defeat the opposing team. The main objective is to destroy the enemy team's Nexus, a structure in their base, while defending your own. It combines elements of strategy, teamwork, and individual skill and is known for its strategic depth and fast-paced action. League of Legends is played by millions of players worldwide and has a thriving esports scene with professional leagues and tournaments.\n",
    "\n",
    "In the competitive environment of League of Legends, players are always looking for ways to improve their chances of winning. Since it's a strategy game, one key element affecting a team's success is the mix of champions they pick. Our aim is to create a model that helps players make better decisions about champion selection and team composition by predicting the likelihood of each team winning based on their chosen champions. This also enables the most dedicated players to dodge an unfavorable matchup before the game begins in such a case where the prediction of their chances of winning are looking less than good.\n",
    "\n",
    "The information about the match is limited to just the champions picked before the game actually begins, so we are going to be using only this information for training our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "There are several datasets available online that contain information about the outcome of the game, champions selected, player stats and much more. There is also the official Riot Games API available, which could be used to gather data from the latest version of the game.\n",
    "\n",
    "For the purpose of this concept, we will be using a dataset from Kaggle. This gives us easy access to a lot of training data, without being limited by the API. While this means that the data is not up to date, it is still a good starting point for our model and useful for evaluating the concept.\n",
    "\n",
    "The dataset [League of Legends- 1 day's worth of solo queue KR](https://www.kaggle.com/datasets/junhachoi/all-ranked-solo-games-on-kr-server-24-hours/) contains information about all ranked matches on the League of Legends Korean Server during the course of 1 day (GMT 2022/07/02 00:00:00 to 2022/07/03 00:00:00). In total, this amounts to over 250.000 matches. The advantage this dataset has over other datasets is that it is very large and one of the most recent ones available. The data is also from a single day, which means that the game version is the same for all matches. This is important because the game is constantly being updated and the balance of champions changes with every patch. This means that the data from older patches is not as useful for training our model.\n",
    "\n",
    "### Data cleaning\n",
    "We will try to clean and improve the dataset by removing some outliers that could negatively impact training. The data cleaning will only be applied to the training data, so we can determine if it had a positive impact. We will also compare the cleaned data with uncleaned data to decide which method is better.\n",
    "\n",
    "The dataset contains multiple values that could be helpful to filter the games. In general, we want to remove games in which the champion selection had a lower impact than usual. We are doing this by finding games that were already uneven from the beginning, since this could indicate that the players' skill level was further apart than normal or that something else that had nothing to do with the champions effected the game, like a player disconnection or not participating. \n",
    "\n",
    "There is a risk that filtering the games in this way will create some form of bias, especially against champions that are stronger in the early stages of the game. We will have to measure how this cleaning effects the performance of our models and adjust the thresholds for filtering games if necessary. \n",
    "\n",
    "The keys we are using to filter games are the following:\n",
    "- gameEndedInEarlySurrender\n",
    "  - This flag indicates whether one team surrendered early.\n",
    "- timePlayed\n",
    "  - We are filtering games that are shorter than 22min, since these games were probably very uneven.\n",
    "- champLevel\n",
    "  - We are filtering games were one or more champions are far below the average, indicating that they were not participating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading game data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2589340it [00:30, 85917.04it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games: 258934\n",
      "train:  233040\n",
      "val:  12947\n",
      "test:  12947\n",
      "\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233040/233040 [10:43<00:00, 362.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 233040\n",
      "Shuffling data...\n",
      "Length of game data: 233040\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233040/233040 [08:56<00:00, 434.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 47555 games from dataset\n",
      "Number of games: 185485\n",
      "Shuffling data...\n",
      "Length of game data: 185485\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12947/12947 [00:37<00:00, 349.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 12947\n",
      "Shuffling data...\n",
      "Length of game data: 12947\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12947/12947 [00:36<00:00, 351.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 12947\n",
      "Shuffling data...\n",
      "Length of game data: 12947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load data used for training\n",
    "import data.kr_24h.convert as convert\n",
    "\n",
    "# load the data, including stats we dont need\n",
    "games_dict = convert.load_raw_csv(file_path=\"data/kr_24h/kr_soloq_24h/sat df.csv\")\n",
    "games = list(games_dict.values())\n",
    "\n",
    "# split into train, val, test\n",
    "train, val, test = convert.split_iterable(games, weights=(90, 5, 5))\n",
    "print(\"train: \", len(train))\n",
    "print(\"val: \", len(val))\n",
    "print(\"test: \", len(test))\n",
    "print()\n",
    "\n",
    "# convert each match into a list of 10 champions and a 1/0 for win/loss of blue team\n",
    "# two copies of train data, one with some matches filtered out\n",
    "train, train_filtered = convert.convert_data(train, filter_matches=False), convert.convert_data(train, filter_matches=True)\n",
    "val = convert.convert_data(val, filter_matches=False)\n",
    "test = convert.convert_data(test, filter_matches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "Before we can start training our model, we need to do some data analysis to get a better understanding of the data. This will help us decide which features to use and how to process them. It can also help us with evaluating the performance of our models later on.\n",
    "\n",
    "### Overall Win Rate\n",
    "The first thing we want to look at is the overall win rate (of the blue side). Since the game is not symmetrical, we can't assume that the win rate is 50%. In fact, during most patches, the blue side (bottom left) has a slightly higher win rate than the red side. This can be explained by several factors, such as the camera angle, the position of the minimap, and the position of the HUD. The blue side also has a slight advantage in champion select, since they get to pick first.\n",
    "\n",
    "This overall win rate gives us a baseline for our model. If our model is not able to beat this baseline, then it is not very useful. The overall win rate is calculated by dividing the number of wins by the total number of matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall win rate here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champion Win Rate\n",
    "Next, we want to look at the win rate of each champion. This gives us an idea of how strong each champion is and how likely they are to win. We can also see which champions are the most popular and which ones are the least popular. With this, we can evaluate the performance of our model and see if it is able to predict the outcome of the game better than just picking the most popular champions. If we match champions with high win rates against champions with low win rates, we can also see if our models are able to predict the outcome of the game correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "train_x, train_y = train[:, :-1], train[:, -1]\n",
    "train_filtered_x, train_filtered_y = train_filtered[:, :-1], train_filtered[:, -1]\n",
    "val_x, val_y = val[:, :-1], val[:, -1]\n",
    "test_x, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# convert y to float and to correct shape\n",
    "def conv_y(y):\n",
    "    y = y.astype(float)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "train_y, train_filtered_y = conv_y(train_y), conv_y(train_filtered_y)\n",
    "val_y, test_y = conv_y(val_y), conv_y(test_y)\n",
    "\n",
    "\n",
    "# convert champion ids to indices and then one-hot encode\n",
    "from champion_dicts import ChampionConverter\n",
    "\n",
    "# see champion_dicts.py for more info\n",
    "# we have to convert the champion ids from the data into indices, since the ids are not contiguous\n",
    "# (some ids are 500+, but there are less than 170 champions)\n",
    "champ_converter = ChampionConverter()\n",
    "\n",
    "def conv_x(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = champ_converter.get_champion_index_from_id(x[i, j])\n",
    "    return x\n",
    "\n",
    "train_x, train_filtered_x = conv_x(train_x), conv_x(train_filtered_x)\n",
    "val_x, test_x = conv_x(val_x), conv_x(test_x)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# one-hot encode the champions, used by simple models\n",
    "CHAMP_NUM = 170 # number of champions, actually a bit less, but this way we could keep same model for more champions\n",
    "def one_hot_encode(x):\n",
    "    one_hot = np.zeros((x.shape[0], x.shape[1], CHAMP_NUM))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            one_hot[i,j,int(x[i,j]-1)] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_x_1hot = one_hot_encode(train_x)\n",
    "train_filtered_x_1hot = one_hot_encode(train_filtered_x)\n",
    "val_x_1hot = one_hot_encode(val_x)\n",
    "test_x_1hot = one_hot_encode(test_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average blue side win chance:  0.5174219018194302\n"
     ]
    }
   ],
   "source": [
    "# calculate average win chance, we should at least beat this :)\n",
    "avg_win_chance = np.average(train_y)\n",
    "print(\"average blue side win chance: \", avg_win_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import stats # for printing and plotting model performance\n",
    "\n",
    "\n",
    "\n",
    "class TrivialModel(tf.keras.Model):\n",
    "    \"\"\"A trivial model that always predicts the average win chance\"\"\"\n",
    "    def __init__(self):\n",
    "        super(TrivialModel, self).__init__()\n",
    "        self.prediction = avg_win_chance\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) > 1:\n",
    "            return np.array([self.prediction]*inputs.shape[0])\n",
    "        return np.array([self.prediction])\n",
    "\n",
    "\n",
    "# baseline model, just some dense layers\n",
    "class BaselineModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu', input_shape=(None,CHAMP_NUM))\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense5 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, (-1, 10, CHAMP_NUM))\n",
    "        # same dense for every player\n",
    "        x = self.dense1(x)\n",
    "        # shape = (-1, 10, 32  )\n",
    "        # flatten\n",
    "        x = tf.reshape(x, (-1, 32*10))\n",
    "        # 3 dense layers, last one is output of (-1, 1)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.dense5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7283/7283 [==============================] - 24s 3ms/step - loss: 0.6907 - accuracy: 0.5284 - val_loss: 0.6903 - val_accuracy: 0.5354\n",
      "Epoch 2/10\n",
      "7283/7283 [==============================] - 22s 3ms/step - loss: 0.6883 - accuracy: 0.5403 - val_loss: 0.6894 - val_accuracy: 0.5399\n",
      "Epoch 3/10\n",
      "5248/7283 [====================>.........] - ETA: 6s - loss: 0.6870 - accuracy: 0.5435"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# train baseline model\n",
    "\n",
    "base_model = BaselineModel()\n",
    "base_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "base_hist = base_model.fit(train_x_1hot, train_y, epochs=10, batch_size=32, validation_data=(val_x_1hot, val_y))\n",
    "\n",
    "# evaluate baseline model\n",
    "print(\"baseline model:\")\n",
    "base_model.evaluate(test_x_1hot, test_y)\n",
    "\n",
    "stats.plot_history(base_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6906 - accuracy: 0.5302 - val_loss: 0.6908 - val_accuracy: 0.5279\n",
      "Epoch 2/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6885 - accuracy: 0.5376 - val_loss: 0.6895 - val_accuracy: 0.5375\n",
      "Epoch 3/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6872 - accuracy: 0.5436 - val_loss: 0.6891 - val_accuracy: 0.5381\n",
      "Epoch 4/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6845 - accuracy: 0.5512 - val_loss: 0.6974 - val_accuracy: 0.5379\n",
      "Epoch 5/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6776 - accuracy: 0.5670 - val_loss: 0.6957 - val_accuracy: 0.5164\n",
      "baseline model filtered:\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.5292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6937665939331055, 0.5292345881462097]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same model, but with some matches filtered out\n",
    "base_model_filtered = BaselineModel()\n",
    "base_model_filtered.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "base_model_filtered.fit(train_filtered_x_1hot, train_filtered_y, epochs=5, batch_size=32, validation_data=(val_x_1hot, val_y))\n",
    "\n",
    "# evaluate baseline model\n",
    "print(\"baseline model filtered:\")\n",
    "base_model_filtered.evaluate(test_x_1hot, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
