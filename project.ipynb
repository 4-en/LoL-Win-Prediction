{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# League of Legends Win Chance Prediction\n",
    "### ML model to predict the outcome of a League of Legends match based on champion selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "League of Legends, often abbreviated as LoL, is a popular online multiplayer video game. It's a competitive 5 versus 5 team-based game in which players control unique champions with special abilities and work together to defeat the opposing team. The main objective is to destroy the enemy team's Nexus, a structure in their base, while defending your own. It combines elements of strategy, teamwork, and individual skill and is known for its strategic depth and fast-paced action. League of Legends is played by millions of players worldwide and has a thriving esports scene with professional leagues and tournaments.\n",
    "\n",
    "In the competitive environment of League of Legends, players are always looking for ways to improve their chances of winning. Since it's a strategy game, one key element affecting a team's success is the mix of champions they pick. Our aim is to create a model that helps players make better decisions about champion selection and team composition by predicting the likelihood of each team winning based on their chosen champions. This also enables the most dedicated players to dodge an unfavorable matchup before the game begins in such a case where the prediction of their chances of winning are looking less than good.\n",
    "\n",
    "The information about the match is limited to just the champions picked before the game actually begins, so we are going to be using only this information for training our model. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "There are several datasets available online that contain information about the outcome of the game, champions selected, player stats and much more. There is also the official Riot Games API available, which could be used to gather data from the latest version of the game.\n",
    "\n",
    "For the purpose of this concept, we will be using a dataset from Kaggle. This gives us easy access to a lot of training data, without being limited by the API. While this means that the data is not up to date, it is still a good starting point for our model and useful for evaluating the concept.\n",
    "\n",
    "The dataset [League of Legends- 1 day's worth of solo queue KR](https://www.kaggle.com/datasets/junhachoi/all-ranked-solo-games-on-kr-server-24-hours/) contains information about all ranked matches on the League of Legends Korean Server during the course of 1 day (GMT 2022/07/02 00:00:00 to 2022/07/03 00:00:00). In total, this amounts to over 250.000 matches. The advantage this dataset has over other datasets is that it is very large and one of the most recent ones available. The data is also from a single day, which means that the game version is the same for all matches. This is important because the game is constantly being updated and the balance of champions changes with every patch. This means that the data from older patches is not as useful for training our model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading game data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2589340it [00:32, 79353.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of games: 258934\n",
      "train:  233040\n",
      "val:  12947\n",
      "test:  12947\n",
      "\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233040/233040 [00:05<00:00, 45391.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 233040\n",
      "Shuffling data...\n",
      "Length of game data: 233040\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 233040/233040 [00:05<00:00, 45678.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 19024 games from dataset\n",
      "Number of games: 214016\n",
      "Shuffling data...\n",
      "Length of game data: 214016\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12947/12947 [00:00<00:00, 54292.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 12947\n",
      "Shuffling data...\n",
      "Length of game data: 12947\n",
      "Converting games...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12947/12947 [00:00<00:00, 51231.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 0 games from dataset\n",
      "Number of games: 12947\n",
      "Shuffling data...\n",
      "Length of game data: 12947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load data used for training\n",
    "import data.kr_24h.convert as convert\n",
    "\n",
    "# load the data, including stats we dont need\n",
    "games_dict = convert.load_raw_csv(file_path=\"data/kr_24h/kr_soloq_24h/sat df.csv\")\n",
    "games = list(games_dict.values())\n",
    "\n",
    "# split into train, val, test\n",
    "train, val, test = convert.split_iterable(games, weights=(90, 5, 5))\n",
    "print(\"train: \", len(train))\n",
    "print(\"val: \", len(val))\n",
    "print(\"test: \", len(test))\n",
    "print()\n",
    "\n",
    "# convert each match into a list of 10 champions and a 1/0 for win/loss of blue team\n",
    "# two copies of train data, one with some matches filtered out\n",
    "train, train_filtered = convert.convert_data(train, filter_matches=False), convert.convert_data(train, filter_matches=True)\n",
    "val = convert.convert_data(val, filter_matches=False)\n",
    "test = convert.convert_data(test, filter_matches=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning\n",
    "can we perform some data cleaning here? Maybe remove matches that were really unbalances/ended early, since then the players were probably the biggest factor in the outcome of the game. Also, we could remove matches where players left the game, since that is not a normal situation and would skew the data.\n",
    "Downside: could negatively affect early game champions, since they are more likely to end the game early and would be removed more often.\n",
    "Datapoints available for each player:\n",
    "- no,gameNo,playerNo,CreationTime,KoreanTime,participantId,teamId,summonerName,gameEndedInEarlySurrender,gameEndedInSurrender,teamEarlySurrendered,win,teamPosition,kills,deaths,assists,objectivesStolen,visionScore,puuid,summonerId,baronKills,bountyLevel,champLevel,championName,damageDealtToBuildings,damageDealtToObjectives,detectorWardsPlaced,doubleKills,dragonKills,firstBloodAssist,firstBloodKill,firstTowerAssist,firstTowerKill,goldEarned,inhibitorKills,inhibitorTakedowns,inhibitorsLost,killingSprees,largestKillingSpree,largestMultiKill,longestTimeSpentLiving,neutralMinionsKilled,objectivesStolenAssists,pentaKills,quadraKills,timeCCingOthers,timePlayed,totalDamageDealt,totalDamageDealtToChampions,totalDamageTaken,totalHeal,totalHealsOnTeammates,totalMinionsKilled,totalTimeCCDealt,totalTimeSpentDead,totalUnitsHealed,tripleKills,unrealKills\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "Before we can start training our model, we need to do some data analysis to get a better understanding of the data. This will help us decide which features to use and how to process them. It can also help us with evaluating the performance of our models later on.\n",
    "\n",
    "### Overall Win Rate\n",
    "The first thing we want to look at is the overall win rate (of the blue side). Since the game is not symmetrical, we can't assume that the win rate is 50%. In fact, during most patches, the blue side (bottom left) has a slightly higher win rate than the red side. This can be explained by several factors, such as the camera angle, the position of the minimap, and the position of the HUD. The blue side also has a slight advantage in champion select, since they get to pick first.\n",
    "\n",
    "This overall win rate gives us a baseline for our model. If our model is not able to beat this baseline, then it is not very useful. The overall win rate is calculated by dividing the number of wins by the total number of matches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate overall win rate here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Champion Win Rate\n",
    "Next, we want to look at the win rate of each champion. This gives us an idea of how strong each champion is and how likely they are to win. We can also see which champions are the most popular and which ones are the least popular. With this, we can evaluate the performance of our model and see if it is able to predict the outcome of the game better than just picking the most popular champions. If we match champions with high win rates against champions with low win rates, we can also see if our models are able to predict the outcome of the game correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into x and y\n",
    "train_x, train_y = train[:, :-1], train[:, -1]\n",
    "train_filtered_x, train_filtered_y = train_filtered[:, :-1], train_filtered[:, -1]\n",
    "val_x, val_y = val[:, :-1], val[:, -1]\n",
    "test_x, test_y = test[:, :-1], test[:, -1]\n",
    "\n",
    "# convert y to float and to correct shape\n",
    "def conv_y(y):\n",
    "    y = y.astype(float)\n",
    "    y = y.reshape(-1, 1)\n",
    "    return y\n",
    "\n",
    "train_y, train_filtered_y = conv_y(train_y), conv_y(train_filtered_y)\n",
    "val_y, test_y = conv_y(val_y), conv_y(test_y)\n",
    "\n",
    "\n",
    "# convert champion ids to indices and then one-hot encode\n",
    "from champion_dicts import ChampionConverter\n",
    "\n",
    "# see champion_dicts.py for more info\n",
    "# we have to convert the champion ids from the data into indices, since the ids are not contiguous\n",
    "# (some ids are 500+, but there are less than 170 champions)\n",
    "champ_converter = ChampionConverter()\n",
    "\n",
    "def conv_x(x):\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            x[i, j] = champ_converter.get_champion_index_from_id(x[i, j])\n",
    "    return x\n",
    "\n",
    "train_x, train_filtered_x = conv_x(train_x), conv_x(train_filtered_x)\n",
    "val_x, test_x = conv_x(val_x), conv_x(test_x)\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# one-hot encode the champions, used by simple models\n",
    "CHAMP_NUM = 170 # number of champions, actually a bit less, but this way we could keep same model for more champions\n",
    "def one_hot_encode(x):\n",
    "    one_hot = np.zeros((x.shape[0], x.shape[1], CHAMP_NUM))\n",
    "    for i in range(x.shape[0]):\n",
    "        for j in range(x.shape[1]):\n",
    "            one_hot[i,j,int(x[i,j]-1)] = 1\n",
    "    return one_hot\n",
    "\n",
    "train_x_1hot = one_hot_encode(train_x)\n",
    "train_filtered_x_1hot = one_hot_encode(train_filtered_x)\n",
    "val_x_1hot = one_hot_encode(val_x)\n",
    "test_x_1hot = one_hot_encode(test_x)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "average blue side win chance:  0.5174219018194302\n"
     ]
    }
   ],
   "source": [
    "# calculate average win chance, we should at least beat this :)\n",
    "avg_win_chance = np.average(train_y)\n",
    "print(\"average blue side win chance: \", avg_win_chance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "class TrivialModel(tf.keras.Model):\n",
    "    \"\"\"A trivial model that always predicts the average win chance\"\"\"\n",
    "    def __init__(self):\n",
    "        super(TrivialModel, self).__init__()\n",
    "        self.prediction = avg_win_chance\n",
    "\n",
    "    def call(self, inputs):\n",
    "        if len(inputs.shape) > 1:\n",
    "            return np.array([self.prediction]*inputs.shape[0])\n",
    "        return np.array([self.prediction])\n",
    "\n",
    "\n",
    "# baseline model, just some dense layers\n",
    "class BaselineModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(BaselineModel, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu', input_shape=(None,CHAMP_NUM))\n",
    "        self.dense2 = tf.keras.layers.Dense(256, activation='relu')\n",
    "        self.dense4 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dense5 = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, (-1, 10, CHAMP_NUM))\n",
    "        # same dense for every player\n",
    "        x = self.dense1(x)\n",
    "        # shape = (-1, 10, 32  )\n",
    "        # flatten\n",
    "        x = tf.reshape(x, (-1, 32*10))\n",
    "        # 3 dense layers, last one is output of (-1, 1)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense4(x)\n",
    "        return self.dense5(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7283/7283 [==============================] - 35s 4ms/step - loss: 0.6906 - accuracy: 0.5288 - val_loss: 0.6899 - val_accuracy: 0.5353\n",
      "Epoch 2/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6884 - accuracy: 0.5390 - val_loss: 0.6893 - val_accuracy: 0.5365\n",
      "Epoch 3/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6869 - accuracy: 0.5429 - val_loss: 0.6906 - val_accuracy: 0.5339\n",
      "Epoch 4/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6838 - accuracy: 0.5510 - val_loss: 0.6915 - val_accuracy: 0.5358\n",
      "Epoch 5/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6763 - accuracy: 0.5685 - val_loss: 0.6973 - val_accuracy: 0.5260\n",
      "Epoch 6/10\n",
      "7283/7283 [==============================] - 28s 4ms/step - loss: 0.6640 - accuracy: 0.5903 - val_loss: 0.7129 - val_accuracy: 0.5275\n",
      "Epoch 7/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6481 - accuracy: 0.6118 - val_loss: 0.7208 - val_accuracy: 0.5174\n",
      "Epoch 8/10\n",
      "7283/7283 [==============================] - 27s 4ms/step - loss: 0.6301 - accuracy: 0.6328 - val_loss: 0.7463 - val_accuracy: 0.5183\n",
      "Epoch 9/10\n",
      "7283/7283 [==============================] - 29s 4ms/step - loss: 0.6113 - accuracy: 0.6528 - val_loss: 0.7708 - val_accuracy: 0.5171\n",
      "Epoch 10/10\n",
      "7283/7283 [==============================] - 30s 4ms/step - loss: 0.5921 - accuracy: 0.6703 - val_loss: 0.7885 - val_accuracy: 0.5144\n",
      "baseline model:\n",
      "405/405 [==============================] - 1s 3ms/step - loss: 0.7859 - accuracy: 0.5115\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7859109044075012, 0.511469841003418]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train baseline model\n",
    "\n",
    "base_model = BaselineModel()\n",
    "base_model.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "base_model.fit(train_x_1hot, train_y, epochs=10, batch_size=32, validation_data=(val_x_1hot, val_y))\n",
    "\n",
    "# evaluate baseline model\n",
    "print(\"baseline model:\")\n",
    "base_model.evaluate(test_x_1hot, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6906 - accuracy: 0.5302 - val_loss: 0.6908 - val_accuracy: 0.5279\n",
      "Epoch 2/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6885 - accuracy: 0.5376 - val_loss: 0.6895 - val_accuracy: 0.5375\n",
      "Epoch 3/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6872 - accuracy: 0.5436 - val_loss: 0.6891 - val_accuracy: 0.5381\n",
      "Epoch 4/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6845 - accuracy: 0.5512 - val_loss: 0.6974 - val_accuracy: 0.5379\n",
      "Epoch 5/5\n",
      "6688/6688 [==============================] - 18s 3ms/step - loss: 0.6776 - accuracy: 0.5670 - val_loss: 0.6957 - val_accuracy: 0.5164\n",
      "baseline model filtered:\n",
      "405/405 [==============================] - 1s 2ms/step - loss: 0.6938 - accuracy: 0.5292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.6937665939331055, 0.5292345881462097]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# same model, but with some matches filtered out\n",
    "base_model_filtered = BaselineModel()\n",
    "base_model_filtered.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "base_model_filtered.fit(train_filtered_x_1hot, train_filtered_y, epochs=5, batch_size=32, validation_data=(val_x_1hot, val_y))\n",
    "\n",
    "# evaluate baseline model\n",
    "print(\"baseline model filtered:\")\n",
    "base_model_filtered.evaluate(test_x_1hot, test_y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
